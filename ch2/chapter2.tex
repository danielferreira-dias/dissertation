% Chapter 2

\chapter{State of the Art: Agentic AI and Small Language Models in Healthcare} % Main chapter title

\label{chap:Chapter2} % For referencing the chapter elsewhere, use \ref{chap:Chapter2} 

%----------------------------------------------------------------------------------------

\section{Research Methodology}


The systematic literature review was conducted following the PRISMA 2020 statement \cite{page2021prisma}, a framework designed to ensure transparency and reproducibility in systematic reviews. This methodology is particularly suited for technological domains such as Agentic AI and medical imaging, where rapid innovation necessitates rigorous evidence synthesis.

The process was structured into four distinct phases: (1) identification of relevant studies through database searching; (2) screening of titles and abstracts based on defined inclusion criteria; (3) eligibility assessment of full-text articles; and (4) qualitative synthesis of the selected studies to answer the defined research questions.

\section{Research Questions}
To structure the systematic review and ensure focused coverage of relevant literature, the following Research Questions (RQs) were formulated. These RQs are specific to the literature review, questions that existing papers will answer to establish the State of the Art.

\textbf{Main Research Question (MRQ):}
\begin{quote}
\textit{To what extent can SLMs achieve performance equivalence with LLMs in specialized domains through Agentic Architectures?}
\end{quote}

\textbf{Sub Research Questions (SRQ):}

\begin{quote}
\textit{What are the limitations of LLMs that justify the architectural shift toward specialized SLMs?}
\end{quote}
\begin{quote}
\textit{What are some of the key characteristics of a Multi-Agent System compared to a Monolithic Single-Agent?}
\end{quote}
\begin{quote}
\textit{What is the comparative efficacy of Retrieval-Augmented Generation (RAG) versus Parameter-Efficient Fine-Tuning (PEFT) for specializing SLMs?}
\end{quote}
\begin{quote}
\textit{What evidence supports using fine-tuned Vision-Language Models over traditional computer vision approaches (CNNs, ViTs)?}
\end{quote}

\section{Databases}
To ensure comprehensive coverage of relevant literature across computer science, medical informatics, and AI research, the following databases were systematically searched:

\begin{table}[ht]
\centering
\caption{Search Strategy: Domains, Databases, and Keywords}
\label{tab:databases}
\begin{tabular}{p{0.30\textwidth}p{0.65\textwidth}}
\hline
\textbf{Database} & \textbf{Description} \\
\hline
\textbf{arXiv} & 
An open-access repository for electronic preprints in the fields of computer science, mathematics, and statistics. It is the primary source for state-of-the-art research in Generative AI, Large Language Models (LLMs), and Small Language Models (SLMs) due to the rapid pace of development in the field. \\
\hline
\textbf{PubMed} & 
A free search engine accessing primarily the MEDLINE database of references and abstracts on life sciences and biomedical topics. It is the gold standard for retrieving peer-reviewed literature on dermatology, telemedicine, and clinical diagnostic methodologies. \\
\hline
\textbf{IEEE Xplore} & 
A digital library providing access to scientific and technical content published by the IEEE. It is essential for technical papers regarding hardware efficiency, edge computing, Multi-Agent System architectures, and optimization techniques for neural networks. \\
\hline
\textbf{ScienceDirect} & 
A large database of scientific and medical research hosted by Elsevier. It contains high-impact, peer-reviewed journals covering both artificial intelligence applications in healthcare and foundational studies in computer vision. \\
\hline
\end{tabular}
\end{table}

\section{Search Terms}
The search strategy utilized boolean logic to combine keywords representing the three core pillars of this dissertation: (1) Generative AI, (2) Artificial Intelligence, (2) Health, and (3) Architecture. The search strings were adapted to the syntax of each database.

\begin{table}[ht]
\centering
\begin{tabular}{p{0.30\textwidth}p{0.65\textwidth}}
\textbf{Domain} & \textbf{Keywords}\\
\hline
Health & \texttt{("Dermatology" OR "Medical" OR "Skin Disease") OR ("Skin" OR "Skin lesion")}\\
\hline
Generative AI & \texttt{("Large Language Models" OR "LLMs") AND ( "Small Language Modles" OR " SLMs")}\\
\hline
Artificial Intelligence & \texttt{("CNN" AND "VLM") OR ("CNN" OR " Vision Language Model") OR ("MLMM" OR "Multimodal Language Model") OR ("CNN" AND "Transformer") OR ("AI" OR "Artificial Intelligence")}\\
\hline
Architecture & \texttt{("MAS" AND "Agentic System") OR ("Multi-Agentic System" AND "Agentic System")}\\
\end{tabular}
\end{table}

\section{Inclusion and Exclusion Criteria}

\subsection{Inclusion Criteria}

Papers were included if they met \textbf{all} of the following conditions:

\begin{enumerate}
    \item \textbf{Publication Date}: Published or made publicly available between January 2018 and December 2025.
  
    \item \textbf{Relevance to AI/ML Models}:
    \begin{itemize}
        \item Must involve Large Language Models (LLMs), Small Language Models (SLMs), Vision-Language Models (VLMs), or Multimodal Models.
        \item OR employ Retrieval-Augmented Generation (RAG) techniques
        \item OR describe Multi-Agent or Agentic AI architectures
    \end{itemize}
    
    \item \textbf{Healthcare or Medical Domain}:
    \begin{itemize}
        \item Papers focusing on dermatology, skin disease classification, or medical question-answering systems are prioritized
    \end{itemize}
    
    \item \textbf{Publication Type}:
    \begin{itemize}
        \item High-quality preprints from established research groups (arXiv, PubMed) if methodologically rigorous
    \end{itemize}
    
    \item \textbf{Language}: Written in English
    
    \item \textbf{Accessibility}: Full text available through institutional access or open access repositories
    
\end{enumerate}

\subsection{Exclusion Criteria}

Papers were excluded if they met \textbf{any} of the following conditions:

\begin{enumerate}
    \item \textbf{Temporal Scope}: Published before January 2020
    \begin{itemize}
        \item Exception: Seminal works cited for background (e.g., foundational Transformer papers) but not included in systematic review
    \end{itemize}
    
    \item \textbf{Duplicate Publications}:
    \begin{itemize}
        \item Conference papers later published as extended journal versions (journal version retained; conference version excluded)
        \item Preprints superseded by peer-reviewed publications (peer-reviewed version retained)
    \end{itemize}
    
    \item \textbf{Non-GenAI Focus}:
    \begin{itemize}
        \item Papers on traditional machine learning (e.g., SVMs, random forests) without LLM/VLM/SLM components
    \end{itemize}
    
    \item \textbf{Language Barrier}: Not written in English and no reliable translation available
    
    \item \textbf{Low Quality Preprints}:
    \begin{itemize}
        \item arXiv/medRxiv papers without peer review that lack methodological rigor, reproducibility details, or validation
    \end{itemize}
\end{enumerate}

\subsection{PRISMA Diagram}

The selection process for the studies included in this systematic review followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020 guidelines. The flow of information through the different phases of the systematic review is detailed in Figure \ref{fig:prisma_diagram}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{images/screenshot_1.png}
    \caption{PRISMA 2020 flow diagram illustrating the selection process of the included studies.}
    \label{fig:prisma_diagram}
\end{figure}

\subsection{Results}

The following section describes how each question was answered according to the information given from the final selected studies.

\subsection{3.1.1 To what extent can SLMs achieve performance equivalence with LLMs in specialized domains through Agentic Architectures?}

The selected papers provide robust empirical evidence that Small Language Models (SLMs), when embedded within agentic or multi-agent architectures, can achieve performance parity or superiority over monolithic Large Language Models (LLMs) in specialized domains. 

The analysis of the selected literature indicates that 100\% of the relevant studies (e.g., Shi et al. 2025; Yilmaz et al. 2025; Wang et al. 2025) report that fine-tuned SLMs outperform generalist models (such as GPT-4o) when the task is decomposed into modular agentic workflows. Specifically, \textit{Shi et al. (2025)} demonstrated in the telecommunications domain that a fine-tuned 8B parameter model within a multi-agent system achieved a six-fold reduction in troubleshooting time while matching the reasoning quality of larger models. Similarly, in the medical domain, \textit{Yilmaz et al. (2025)} and \textit{Wang et al. (2025)} showed that decomposing diagnostic tasks into retrieval and reasoning steps allows smaller models (Llama-3.2-11B and similar architectures) to surpass commercial LLM benchmarks on datasets like DERM12345 and PubMedQA.

Regarding publication venues and impact, the selected studies reflect high-quality technical contributions. Several works have been published or accepted in high-impact venues such as \textit{Nature Medicine} (Singhal et al. 2025), \textit{NeurIPS} (Wang et al. 2025), and major archives for computer science research (arXiv cs.AI). The architectural validation is further supported by \textit{Tian et al. (2025)}, who provided a systematic evaluation on the GPQA-Diamond benchmark, proving that multi-agent orchestration consistently exceeds single-model baselines regardless of model size. This indicates a consensus in the recent literature (2024--2025) that architectural orchestration is a more significant determinant of accuracy than parameter count in specialized applications.

\vspace{1cm}

\begin{table}[h]
\centering
\caption{Quality appraisal questions for selected primary studies}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|p{1cm}|p{10cm}|p{3.5cm}|}
\hline
\textbf{Ref.} & \textbf{Question} & \textbf{Answer Criteria} \\
\hline
QA1 & Does the study provide a direct quantitative comparison between the proposed SLM-based system and a State-of-the-Art (SOTA) LLM (e.g., GPT-4)? & Yes (+1); No (0) \\
\hline
QA2 & Does the study explicitly employ a Multi-Agent System (MAS), Agentic Workflow, or composite architecture (e.g., RAG + Fine-tuning) rather than a single inference pass? & Yes (+1); No (0) \\
\hline
QA3 & Is the performance evaluation conducted within a specialized high-stakes domain (e.g., Healthcare, Telecommunications, Law) requiring domain grounding? & Yes (+1); No (0) \\
\hline
QA4 & Does the study provide open-source code, datasets, or reproducible architectural blueprints (e.g., GitHub repositories or detailed prompt frameworks)? & Yes (+1); Partial (+0.5); No (0) \\
\hline
\end{tabular}
\label{tab:quality_appraisal}
\end{table}


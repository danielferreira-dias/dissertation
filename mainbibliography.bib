%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Luiz Faria at 2022-01-20 21:26:40 +0000 


%% Saved with string encoding Unicode (UTF-8) 



@article{faria2009,
	author = {L. Faria and A. Silva and Z. Vale and A. Marques},
	date-added = {2022-01-20 21:25:07 +0000},
	date-modified = {2022-01-20 21:26:40 +0000},
	journal = {IEEE Transactions on Learning Technologies},
	number = {2},
	pages = {135--147},
	title = {Training control centers' operators in incident diagnosis and power restoration using intelligent tutoring systems},
	volume = {2},
	year = {2009}}

@article{marreiros2010,
	author = {G. Marreiros and R. Santos and C. Ramos and J. Neves},
	date-added = {2022-01-20 21:23:27 +0000},
	date-modified = {2022-01-20 21:24:57 +0000},
	journal = {IEEE Intelligent Systems},
	number = {2},
	pages = {31--39},
	title = {Context-aware emotion-based model for group decision making},
	volume = {25},
	year = {2010}}

@article{ramos2008,
	author = {C. Ramos and J. C. Augusto and D. Shapiro},
	date-added = {2022-01-20 21:13:12 +0000},
	date-modified = {2022-01-20 21:14:29 +0000},
	journal = {IEEE Intelligent Systems},
	number = {2},
	pages = {15--18},
	title = {Ambient intelligence -- the next step for artificial intelligence},
	volume = {23},
	year = {2008}}

@article{Reference1,
	abstract = {We have developed an enhanced Littrow configuration extended cavity diode laser (ECDL) that can be tuned without changing the direction of the output beam. The output of a conventional Littrow ECDL is reflected from a plane mirror fixed parallel to the tuning diffraction grating. Using a free-space Michelson wavemeter to measure the laser wavelength, we can tune the laser over a range greater than 10 nm without any alteration of alignment.},
	author = {C. J. Hawthorn and K. P. Weber and R. E. Scholten},
	journal = {Review of Scientific Instruments},
	month = {12},
	number = {12},
	numpages = {3},
	pages = {4477--4479},
	title = {Littrow Configuration Tunable External Cavity Diode Laser with Fixed Direction Output Beam},
	url = {http://link.aip.org/link/?RSI/72/4477/1},
	volume = {72},
	year = {2001},
	bdsk-url-1 = {http://link.aip.org/link/?RSI/72/4477/1}}

@article{Reference3,
	abstract = {Operating a laser diode in an extended cavity which provides frequency-selective feedback is a very effective method of reducing the laser's linewidth and improving its tunability. We have developed an extremely simple laser of this type, built from inexpensive commercial components with only a few minor modifications. A 780~nm laser built to this design has an output power of 80~mW, a linewidth of 350~kHz, and it has been continuously locked to a Doppler-free rubidium transition for several days.},
	author = {A. S. Arnold and J. S. Wilson and M. G. Boshier and J. Smith},
	journal = {Review of Scientific Instruments},
	month = {3},
	number = {3},
	numpages = {4},
	pages = {1236--1239},
	title = {A Simple Extended-Cavity Diode Laser},
	url = {http://link.aip.org/link/?RSI/69/1236/1},
	volume = {69},
	year = {1998},
	bdsk-url-1 = {http://link.aip.org/link/?RSI/69/1236/1}}

@article{Reference2,
	abstract = {We present a review of the use of diode lasers in atomic physics with an extensive list of references. We discuss the relevant characteristics of diode lasers and explain how to purchase and use them. We also review the various techniques that have been used to control and narrow the spectral outputs of diode lasers. Finally we present a number of examples illustrating the use of diode lasers in atomic physics experiments. Review of Scientific Instruments is copyrighted by The American Institute of Physics.},
	author = {Carl E. Wieman and Leo Hollberg},
	journal = {Review of Scientific Instruments},
	keywords = {Diode Laser},
	month = {1},
	number = {1},
	numpages = {20},
	pages = {1--20},
	title = {Using Diode Lasers for Atomic Physics},
	url = {http://link.aip.org/link/?RSI/62/1/1},
	volume = {62},
	year = {1991},
	bdsk-url-1 = {http://link.aip.org/link/?RSI/62/1/1}}


%% ============================================================================
%% DISSERTATION REFERENCES - Small Language Models & Multi-Agent Systems
%% Added from PRISMA Literature Review - January 2026
%% ============================================================================

%% --------------------------------------------------------------------------
%% Theme 1: SLMs as Future of Agentic AI (Foundation)
%% --------------------------------------------------------------------------

@article{belcak2025slm,
	author = {Peter Belcak and Greg Heinrich and Shizhe Diao and Yonggan Fu and Xin Dong and Saurav Muralidharan and Yingyan Celine Lin and Pavlo Molchanov},
	title = {Small Language Models are the Future of Agentic AI},
	journal = {arXiv preprint arXiv:2506.02153},
	year = {2025},
	eprint = {2506.02153},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL},
	note = {NVIDIA Research}}

@article{lu2025slmsurvey,
	author = {Faisal Tareque Shohan and Aurko Roy and Jing Huang and Wei Xu},
	title = {A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with {LLMs}, and Trustworthiness},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	year = {2025},
	doi = {10.1145/3768165},
	publisher = {ACM}}

%% --------------------------------------------------------------------------
%% Theme 2: Fine-tuned SLMs Outperforming LLMs
%% --------------------------------------------------------------------------

@article{bucher2024finetuned,
	author = {Martin Juan Jos{\'e} Bucher and Marco Martini},
	title = {Fine-Tuned `Small' {LLMs} (Still) Significantly Outperform Zero-Shot Generative {AI} Models in Text Classification},
	journal = {arXiv preprint arXiv:2406.08660},
	year = {2024},
	eprint = {2406.08660},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}}

@article{liu2024toolcalling,
	author = {Yuxiang Zhang and Yachen Yan and Haolong Li and Deyu Zhou},
	title = {Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning},
	journal = {arXiv preprint arXiv:2512.15943},
	year = {2024},
	eprint = {2512.15943},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL},
	note = {AWS Research}}

@article{suarez2024instructionft,
	author = {Perez Suarez and others},
	title = {A Comparative Analysis of Instruction Fine-Tuning Large Language Models for Financial Text Classification},
	journal = {ACM Transactions on Management Information Systems},
	year = {2024},
	doi = {10.1145/3706119},
	publisher = {ACM}}

@article{clever2024,
	author = {Chen, Wei and others},
	title = {Clinical Large Language Model Evaluation by Expert Review ({CLEVER}): Framework Development and Validation},
	journal = {Journal of Medical Internet Research},
	year = {2024},
	doi = {10.2196/12677871},
	note = {Fine-tuned 8B MedS model outperformed GPT-4o}}

%% --------------------------------------------------------------------------
%% Theme 3: Multi-Agent Systems & SLM Collaboration
%% --------------------------------------------------------------------------

@article{tran2025multiagent,
	author = {Khanh-Tung Tran and Dung Dao and Minh-Duong Nguyen and Quoc-Viet Pham and Barry O'Sullivan and Hoang D. Nguyen},
	title = {Multi-Agent Collaboration Mechanisms: A Survey of {LLMs}},
	journal = {arXiv preprint arXiv:2501.06322},
	year = {2025},
	eprint = {2501.06322},
	archiveprefix = {arXiv},
	primaryclass = {cs.MA}}

@article{wang2025slmllmcollab,
	author = {Wang, Shujin and others},
	title = {A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness},
	journal = {arXiv preprint arXiv:2510.13890},
	year = {2025},
	eprint = {2510.13890},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}}

@article{gabriel2024agenticsystems,
	author = {Adrian Garret Gabriel and Alaa Alameer Ahmad and Shankar Kumar Jeyakumar},
	title = {Advancing Agentic Systems: Dynamic Task Decomposition, Tool Integration and Evaluation using Novel Metrics and Dataset},
	journal = {arXiv preprint arXiv:2410.22457},
	year = {2024},
	eprint = {2410.22457},
	archiveprefix = {arXiv},
	primaryclass = {cs.AI},
	note = {NeurIPS 2024}}

@article{hong2024metagpt,
	author = {Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Ceyao Zhang and Jinlin Wang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and J{\"u}rgen Schmidhuber},
	title = {{MetaGPT}: Meta Programming for A Multi-Agent Collaborative Framework},
	journal = {arXiv preprint arXiv:2308.00352},
	year = {2024},
	note = {ICLR 2024}}

@article{chen2024autoagents,
	author = {Guangyao Chen and Siwei Dong and Yu Shu and Ge Zhang and Jaward Sesay and B{\"o}rje F. Karlsson and Jie Fu and Yemin Shi},
	title = {{AutoAgents}: A Framework for Automatic Agent Generation},
	booktitle = {Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (IJCAI-24)},
	year = {2024},
	pages = {890--898}}

@article{li2025toollearning,
	author = {Li, Zhongxiang and others},
	title = {{LLM}-Based Agents for Tool Learning: A Survey},
	journal = {Data Science and Engineering},
	year = {2025},
	doi = {10.1007/s41019-025-00296-9},
	publisher = {Springer}}

%% --------------------------------------------------------------------------
%% Theme 4: Medical AI & Dermatology Applications
%% --------------------------------------------------------------------------

@article{zhou2024skingpt4,
	author = {Juexiao Zhou and Xiaonan He and Liyuan Sun and Jiannan Xu and Xiuying Chen and Yuetan Chu and Longxi Zhou and Xingyu Liao and Bin Zhang and Xin Gao},
	title = {Pre-trained multimodal large language model enhances dermatological diagnosis using {SkinGPT-4}},
	journal = {Nature Communications},
	volume = {15},
	number = {1},
	pages = {5649},
	year = {2024},
	doi = {10.1038/s41467-024-50043-3},
	publisher = {Nature Publishing Group}}

@article{liu2025panderm,
	author = {Liu, Chang and others},
	title = {{PanDerm}: A multimodal vision foundation model for clinical dermatology},
	journal = {Nature Medicine},
	year = {2025},
	doi = {10.1038/s41591-025-03747-y},
	publisher = {Nature Publishing Group},
	note = {Pretrained on 2M+ real-world images from 11 institutions}}

@article{yang2025derm1m,
	author = {Yang, Sen and others},
	title = {{Derm1M}: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology},
	journal = {arXiv preprint arXiv:2503.14911},
	year = {2025},
	eprint = {2503.14911},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}}

@article{chen2025dermatollama,
	author = {Chen, Yuxuan and others},
	title = {Resource-efficient medical vision language model for dermatology via a synthetic data generation framework ({SCALEMED}/{DermatoLlama})},
	journal = {medRxiv preprint},
	year = {2025},
	doi = {10.1101/2025.05.17.25327785}}

@inproceedings{dermai2024,
	author = {Santos, Ricardo and others},
	title = {{DermAI}: A Chatbot Assistant for Skin Lesion Diagnosis Using Vision and Large Language Models},
	booktitle = {ACCV 2024 Workshops},
	year = {2024},
	publisher = {Springer},
	doi = {10.1007/978-981-96-2641-0_20}}

@article{patricio2024conceptvlm,
	author = {Cristiano Patr{\'i}cio and Lu{\'i}s F. Teixeira and Jo{\~a}o C. Neves},
	title = {Towards Concept-based Interpretability of Skin Lesion Diagnosis using Vision-Language Models},
	journal = {arXiv preprint arXiv:2311.14339},
	year = {2024},
	eprint = {2311.14339},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV},
	note = {IEEE ISBI 2024}}

@article{ferrara2024aiderm,
	author = {Ferrara, Giovanni and others},
	title = {The Use of Artificial Intelligence for Skin Disease Diagnosis in Primary Care Settings: A Systematic Review},
	journal = {Healthcare},
	volume = {12},
	number = {12},
	pages = {1192},
	year = {2024},
	doi = {10.3390/healthcare12121192},
	publisher = {MDPI}}

@article{orenstein2023aiskinlesion,
	author = {Orenstein, Nicolas and others},
	title = {Exploring the potential of artificial intelligence in improving skin lesion diagnosis in primary care},
	journal = {Scientific Reports},
	volume = {13},
	pages = {4569},
	year = {2023},
	doi = {10.1038/s41598-023-31340-1},
	publisher = {Nature Publishing Group}}

%% --------------------------------------------------------------------------
%% Theme 5: Knowledge Distillation Techniques
%% --------------------------------------------------------------------------

@article{xu2024kdsurvey,
	author = {Xiaohan Xu and Ming Li and Chongyang Tao and Tao Shen and Reynold Cheng and Jinyang Li and Can Xu and Dacheng Tao and Tianyi Zhou},
	title = {A Survey on Knowledge Distillation of Large Language Models},
	journal = {arXiv preprint arXiv:2402.13116},
	year = {2024},
	eprint = {2402.13116},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}}

@inproceedings{gu2024minillm,
	author = {Yuxian Gu and Li Dong and Furu Wei and Minlie Huang},
	title = {{MiniLLM}: Knowledge Distillation of Large Language Models},
	booktitle = {International Conference on Learning Representations (ICLR)},
	year = {2024},
	eprint = {2306.08543},
	archiveprefix = {arXiv}}

@article{acmkdsurvey2024,
	author = {Wang, Cheng and others},
	title = {Survey on Knowledge Distillation for {LLMs}: Methods, Evaluation, and Application},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	year = {2024},
	doi = {10.1145/3699518},
	publisher = {ACM}}

@inproceedings{chen2024evokd,
	author = {Chen, Jiaxi and others},
	title = {Evolving Knowledge Distillation with Large Language Models},
	booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
	year = {2024},
	pages = {6757--6769},
	publisher = {ACL}}

%% --------------------------------------------------------------------------
%% Theme 6: RAG with Small Language Models
%% --------------------------------------------------------------------------

@article{wang2025ragbestpractices,
	author = {Wang, Xiaohua and others},
	title = {Enhancing Retrieval-Augmented Generation: A Study of Best Practices},
	journal = {arXiv preprint arXiv:2501.07391},
	year = {2025},
	eprint = {2501.07391},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}}

@article{gao2024ragsurvey,
	author = {Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
	title = {Retrieval-Augmented Generation for Large Language Models: A Survey},
	journal = {arXiv preprint arXiv:2312.10997},
	year = {2024},
	eprint = {2312.10997},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}}

@article{chen2025ragsystematic,
	author = {Chen, Bing and others},
	title = {A Systematic Review of Key {RAG} Systems: Progress, Gaps, and Future Directions},
	journal = {arXiv preprint arXiv:2507.18910},
	year = {2025},
	eprint = {2507.18910},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}}

@article{zhang2025dragon,
	author = {Zhang, Fei and others},
	title = {{DRAGON}: Efficient Distributed Retrieval-Augmented Generation for Enhancing On-Device {LM} Inference},
	journal = {arXiv preprint arXiv:2504.11197},
	year = {2025},
	eprint = {2504.11197},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}}

%% --------------------------------------------------------------------------
%% Theme 7: Vision-Language Models for Medical Imaging
%% --------------------------------------------------------------------------

@article{kim2025vlmmedical,
	author = {Kim, Seung and others},
	title = {Vision-language foundation models for medical imaging: a review of current practices and innovations},
	journal = {Biomedical Engineering Letters},
	year = {2025},
	doi = {10.1007/s13534-025-00484-6},
	publisher = {Springer}}

@article{chen2025vlmfusion,
	author = {Chen, Zehao and others},
	title = {Vision-Language Models in medical image analysis: From simple fusion to general large models},
	journal = {Information Fusion},
	year = {2025},
	doi = {10.1016/j.inffus.2025.102860},
	publisher = {Elsevier}}

@article{liu2025vlmmetaanalysis,
	author = {Liu, Jia and others},
	title = {Visual-language foundation models in medical imaging: Systematic review and meta-analysis of diagnostic and analytical applications},
	journal = {Computer Methods and Programs in Biomedicine},
	year = {2025},
	doi = {10.1016/j.cmpb.2025.108221},
	publisher = {Elsevier}}

@article{yao2025minicpmv,
	author = {Yao, Yuan and others},
	title = {{MiniCPM-V}: Efficient {GPT-4V} level multimodal large language model for deployment on edge devices},
	journal = {Nature Communications},
	year = {2025},
	doi = {10.1038/s41467-025-61040-5},
	publisher = {Nature Publishing Group}}

@article{monshi2024vlmreportgen,
	author = {Monshi, Momina and others},
	title = {Vision-language models for medical report generation and visual question answering: a review},
	journal = {Frontiers in Artificial Intelligence},
	volume = {7},
	pages = {1430984},
	year = {2024},
	doi = {10.3389/frai.2024.1430984},
	publisher = {Frontiers}}

%% --------------------------------------------------------------------------
%% Theme 8: Edge Deployment & Privacy-Preserving AI
%% --------------------------------------------------------------------------

@article{xu2025edgellm,
	author = {Xu, Zhiyuan and others},
	title = {A Review on Edge Large Language Models: Design, Execution, and Applications},
	journal = {ACM Computing Surveys},
	year = {2025},
	doi = {10.1145/3719664},
	publisher = {ACM}}

@article{li2025cognitiveedge,
	author = {Li, Haoran and others},
	title = {Cognitive Edge Computing: A Comprehensive Survey},
	journal = {arXiv preprint arXiv:2501.03265},
	year = {2025},
	eprint = {2501.03265},
	archiveprefix = {arXiv},
	primaryclass = {cs.DC}}

@article{zhang2025vlmedge,
	author = {Zhang, Wei and others},
	title = {Vision-Language Models for Edge Networks: A Comprehensive Survey},
	journal = {arXiv preprint arXiv:2502.07855},
	year = {2025},
	eprint = {2502.07855},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}}

@article{rahman2025edgeai,
	author = {Rahman, Md and others},
	title = {Edge-{AI} integrated secure wireless {IoT} architecture for real time healthcare monitoring and federated anomaly detection},
	journal = {Scientific Reports},
	volume = {15},
	pages = {30150},
	year = {2025},
	doi = {10.1038/s41598-025-30150-x},
	publisher = {Nature Publishing Group}}

%% --------------------------------------------------------------------------
%% Theme 9: Parameter-Efficient Fine-Tuning (PEFT)
%% --------------------------------------------------------------------------

@article{gema2024clinicallora,
	author = {Aryo Pradipta Gema and Pasquale Minervini and Luke Daines and Tom Hope and Beatrice Alex},
	title = {Parameter-Efficient Fine-Tuning of {LLaMA} for the Clinical Domain},
	journal = {arXiv preprint arXiv:2307.03042},
	year = {2024},
	eprint = {2307.03042},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}}

@article{he2024pefomed,
	author = {Jinlong He and Pengfei Li and Gang Liu and Genrong He and Zhaolin Chen and Shenjun Zhong},
	title = {{PeFoMed}: Parameter Efficient Fine-tuning of Multimodal Large Language Models for Medical Imaging},
	journal = {arXiv preprint arXiv:2401.02797},
	year = {2024},
	eprint = {2401.02797},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}}

@article{han2024peftsurvey,
	author = {Zeyu Han and Chao Gao and Jinyang Liu and Jeff Zhang and Sai Qian Zhang},
	title = {Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey},
	journal = {arXiv preprint arXiv:2403.14608},
	year = {2024},
	eprint = {2403.14608},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}}

@article{wang2025peftmethodologies,
	author = {Wang, Benyuan and others},
	title = {Parameter-efficient fine-tuning in large language models: a survey of methodologies},
	journal = {Artificial Intelligence Review},
	year = {2025},
	doi = {10.1007/s10462-025-11236-4},
	publisher = {Springer}}

@article{peftmedical2024,
	author = {Alshareef, Ammar and others},
	title = {Improving Medical Abstract Classification Using {PEFT-LoRA} Fine-Tuned Large and Small Language Models},
	journal = {International Journal of Computing and Engineering},
	year = {2024},
	doi = {10.47941/ijce.2374}}

%% --------------------------------------------------------------------------
%% Theme 10: Model Quantization & Optimization
%% --------------------------------------------------------------------------

@article{nvidia2024ptq,
	author = {{NVIDIA Corporation}},
	title = {Optimizing {LLMs} for Performance and Accuracy with Post-Training Quantization},
	journal = {NVIDIA Technical Blog},
	year = {2024},
	url = {https://developer.nvidia.com/blog/optimizing-llms-for-performance-and-accuracy-with-post-training-quantization/}}

@article{liu2024quantization,
	author = {Liu, Zechun and others},
	title = {Optimizing Large Language Models through Quantization: A Comparative Analysis of {PTQ} and {QAT} Techniques},
	journal = {arXiv preprint arXiv:2411.06084},
	year = {2024},
	eprint = {2411.06084},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}}

@inproceedings{zhao2024atom,
	author = {Zhao, Yilong and others},
	title = {{ATOM}: Low-bit Quantization for Efficient and Accurate {LLM} Serving},
	booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
	year = {2024}}

%% --------------------------------------------------------------------------
%% Theme 11: Hallucination Mitigation in Medical AI
%% --------------------------------------------------------------------------

@article{wang2025hallucination,
	author = {Wang, Jiacheng and others},
	title = {A framework to assess clinical safety and hallucination rates of {LLMs} for medical text summarisation},
	journal = {npj Digital Medicine},
	year = {2025},
	doi = {10.1038/s41746-025-01670-7},
	publisher = {Nature Publishing Group}}

@article{chen2025medicalhallucination,
	author = {Chen, Yutong and others},
	title = {Medical Hallucination in Foundation Models and Their Impact on Healthcare},
	journal = {medRxiv preprint},
	year = {2025},
	doi = {10.1101/2025.02.28.25323115}}

@article{zhang2025hallucinationsurvey,
	author = {Zhang, Yue and others},
	title = {A Comprehensive Survey of Hallucination in {LLMs}: Causes, Detection, and Mitigation},
	journal = {arXiv preprint arXiv:2510.06265},
	year = {2025},
	eprint = {2510.06265},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}}

%% --------------------------------------------------------------------------
%% Theme 12: Benchmarks & Evaluation
%% --------------------------------------------------------------------------

@misc{openmedicalllm2024,
	author = {{Hugging Face}},
	title = {The Open Medical-{LLM} Leaderboard: Benchmarking Large Language Models in Healthcare},
	year = {2024},
	url = {https://huggingface.co/blog/leaderboard-medicalllm}}

@inproceedings{wang2024mmlupro,
	author = {Wang, Yubo and others},
	title = {{MMLU-Pro}: A More Robust and Challenging Multi-Task Language Understanding Benchmark},
	booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
	year = {2024}}

@article{sharma2025medllmeval,
	author = {Sharma, Pranav and others},
	title = {Evaluating Large Reasoning Model Performance on Complex Medical Scenarios In the {MMLU-Pro} Benchmark},
	journal = {medRxiv preprint},
	year = {2025},
	doi = {10.1101/2025.04.07.25325385}}

@article{chen2025medicalllms,
	author = {Chen, Lei and others},
	title = {Medical {LLMs}: Fine-Tuning vs. Retrieval-Augmented Generation},
	journal = {Bioengineering},
	volume = {12},
	number = {7},
	pages = {687},
	year = {2025},
	doi = {10.3390/bioengineering12070687},
	publisher = {MDPI}}

@article{kim2025medicineedge,
	author = {Kim, Sungwon and others},
	title = {Medicine on the Edge: Comparative Performance Analysis of On-Device {LLMs} for Clinical Reasoning},
	journal = {arXiv preprint arXiv:2502.08954},
	year = {2025},
	eprint = {2502.08954},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}}

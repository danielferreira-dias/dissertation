%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Luiz Faria at 2022-01-20 21:26:40 +0000 


%% Saved with string encoding Unicode (UTF-8) 



@article{faria2009,
	author = {L. Faria and A. Silva and Z. Vale and A. Marques},
	date-added = {2022-01-20 21:25:07 +0000},
	date-modified = {2022-01-20 21:26:40 +0000},
	journal = {IEEE Transactions on Learning Technologies},
	number = {2},
	pages = {135--147},
	title = {Training control centers' operators in incident diagnosis and power restoration using intelligent tutoring systems},
	volume = {2},
	year = {2009}}

@article{marreiros2010,
	author = {G. Marreiros and R. Santos and C. Ramos and J. Neves},
	date-added = {2022-01-20 21:23:27 +0000},
	date-modified = {2022-01-20 21:24:57 +0000},
	journal = {IEEE Intelligent Systems},
	number = {2},
	pages = {31--39},
	title = {Context-aware emotion-based model for group decision making},
	volume = {25},
	year = {2010}}

@article{ramos2008,
	author = {C. Ramos and J. C. Augusto and D. Shapiro},
	date-added = {2022-01-20 21:13:12 +0000},
	date-modified = {2022-01-20 21:14:29 +0000},
	journal = {IEEE Intelligent Systems},
	number = {2},
	pages = {15--18},
	title = {Ambient intelligence -- the next step for artificial intelligence},
	volume = {23},
	year = {2008}}

@article{Reference1,
	abstract = {We have developed an enhanced Littrow configuration extended cavity diode laser (ECDL) that can be tuned without changing the direction of the output beam. The output of a conventional Littrow ECDL is reflected from a plane mirror fixed parallel to the tuning diffraction grating. Using a free-space Michelson wavemeter to measure the laser wavelength, we can tune the laser over a range greater than 10 nm without any alteration of alignment.},
	author = {C. J. Hawthorn and K. P. Weber and R. E. Scholten},
	journal = {Review of Scientific Instruments},
	month = {12},
	number = {12},
	numpages = {3},
	pages = {4477--4479},
	title = {Littrow Configuration Tunable External Cavity Diode Laser with Fixed Direction Output Beam},
	url = {http://link.aip.org/link/?RSI/72/4477/1},
	volume = {72},
	year = {2001},
	bdsk-url-1 = {http://link.aip.org/link/?RSI/72/4477/1}}

@article{Reference3,
	abstract = {Operating a laser diode in an extended cavity which provides frequency-selective feedback is a very effective method of reducing the laser's linewidth and improving its tunability. We have developed an extremely simple laser of this type, built from inexpensive commercial components with only a few minor modifications. A 780~nm laser built to this design has an output power of 80~mW, a linewidth of 350~kHz, and it has been continuously locked to a Doppler-free rubidium transition for several days.},
	author = {A. S. Arnold and J. S. Wilson and M. G. Boshier and J. Smith},
	journal = {Review of Scientific Instruments},
	month = {3},
	number = {3},
	numpages = {4},
	pages = {1236--1239},
	title = {A Simple Extended-Cavity Diode Laser},
	url = {http://link.aip.org/link/?RSI/69/1236/1},
	volume = {69},
	year = {1998},
	bdsk-url-1 = {http://link.aip.org/link/?RSI/69/1236/1}}

@article{Reference2,
	abstract = {We present a review of the use of diode lasers in atomic physics with an extensive list of references. We discuss the relevant characteristics of diode lasers and explain how to purchase and use them. We also review the various techniques that have been used to control and narrow the spectral outputs of diode lasers. Finally we present a number of examples illustrating the use of diode lasers in atomic physics experiments. Review of Scientific Instruments is copyrighted by The American Institute of Physics.},
	author = {Carl E. Wieman and Leo Hollberg},
	journal = {Review of Scientific Instruments},
	keywords = {Diode Laser},
	month = {1},
	number = {1},
	numpages = {20},
	pages = {1--20},
	title = {Using Diode Lasers for Atomic Physics},
	url = {http://link.aip.org/link/?RSI/62/1/1},
	volume = {62},
	year = {1991},
	bdsk-url-1 = {http://link.aip.org/link/?RSI/62/1/1}}


%% ============================================================================
%% DISSERTATION REFERENCES - Small Language Models & Multi-Agent Systems
%% Added from PRISMA Literature Review - January 2026
%% ============================================================================

%% --------------------------------------------------------------------------
%% Theme 1: SLMs as Future of Agentic AI (Foundation)
%% --------------------------------------------------------------------------

@article{belcak2025slm,
	author = {Peter Belcak and Greg Heinrich and Shizhe Diao and Yonggan Fu and Xin Dong and Saurav Muralidharan and Yingyan Celine Lin and Pavlo Molchanov},
	title = {Small Language Models are the Future of Agentic AI},
	journal = {arXiv preprint arXiv:2506.02153},
	year = {2025},
	eprint = {2506.02153},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL},
	note = {NVIDIA Research}}

@article{lu2025slmsurvey,
	author = {Faisal Tareque Shohan and Aurko Roy and Jing Huang and Wei Xu},
	title = {A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with {LLMs}, and Trustworthiness},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	year = {2025},
	doi = {10.1145/3768165},
	publisher = {ACM}}

%% --------------------------------------------------------------------------
%% Theme 2: Fine-tuned SLMs Outperforming LLMs
%% --------------------------------------------------------------------------

@article{bucher2024finetuned,
	author = {Martin Juan Jos{\'e} Bucher and Marco Martini},
	title = {Fine-Tuned `Small' {LLMs} (Still) Significantly Outperform Zero-Shot Generative {AI} Models in Text Classification},
	journal = {arXiv preprint arXiv:2406.08660},
	year = {2024},
	eprint = {2406.08660},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}}

@article{liu2024toolcalling,
	author = {Yuxiang Zhang and Yachen Yan and Haolong Li and Deyu Zhou},
	title = {Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning},
	journal = {arXiv preprint arXiv:2512.15943},
	year = {2024},
	eprint = {2512.15943},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL},
	note = {AWS Research}}

@article{suarez2024instructionft,
	author = {Perez Suarez and others},
	title = {A Comparative Analysis of Instruction Fine-Tuning Large Language Models for Financial Text Classification},
	journal = {ACM Transactions on Management Information Systems},
	year = {2024},
	doi = {10.1145/3706119},
	publisher = {ACM}}

@article{clever2024,
	author = {Chen, Wei and others},
	title = {Clinical Large Language Model Evaluation by Expert Review ({CLEVER}): Framework Development and Validation},
	journal = {Journal of Medical Internet Research},
	year = {2024},
	doi = {10.2196/12677871},
	note = {Fine-tuned 8B MedS model outperformed GPT-4o}}

%% --------------------------------------------------------------------------
%% Theme 3: Multi-Agent Systems & SLM Collaboration
%% --------------------------------------------------------------------------

@article{tran2025multiagent,
	author = {Khanh-Tung Tran and Dung Dao and Minh-Duong Nguyen and Quoc-Viet Pham and Barry O'Sullivan and Hoang D. Nguyen},
	title = {Multi-Agent Collaboration Mechanisms: A Survey of {LLMs}},
	journal = {arXiv preprint arXiv:2501.06322},
	year = {2025},
	eprint = {2501.06322},
	archiveprefix = {arXiv},
	primaryclass = {cs.MA}}

@article{wang2025slmllmcollab,
	author = {Wang, Shujin and others},
	title = {A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness},
	journal = {arXiv preprint arXiv:2510.13890},
	year = {2025},
	eprint = {2510.13890},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}}

@article{gabriel2024agenticsystems,
	author = {Adrian Garret Gabriel and Alaa Alameer Ahmad and Shankar Kumar Jeyakumar},
	title = {Advancing Agentic Systems: Dynamic Task Decomposition, Tool Integration and Evaluation using Novel Metrics and Dataset},
	journal = {arXiv preprint arXiv:2410.22457},
	year = {2024},
	eprint = {2410.22457},
	archiveprefix = {arXiv},
	primaryclass = {cs.AI},
	note = {NeurIPS 2024}}

@article{hong2024metagpt,
	author = {Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Ceyao Zhang and Jinlin Wang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and J{\"u}rgen Schmidhuber},
	title = {{MetaGPT}: Meta Programming for A Multi-Agent Collaborative Framework},
	journal = {arXiv preprint arXiv:2308.00352},
	year = {2024},
	note = {ICLR 2024}}

@article{chen2024autoagents,
	author = {Guangyao Chen and Siwei Dong and Yu Shu and Ge Zhang and Jaward Sesay and B{\"o}rje F. Karlsson and Jie Fu and Yemin Shi},
	title = {{AutoAgents}: A Framework for Automatic Agent Generation},
	booktitle = {Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (IJCAI-24)},
	year = {2024},
	pages = {890--898}}

@article{li2025toollearning,
	author = {Li, Zhongxiang and others},
	title = {{LLM}-Based Agents for Tool Learning: A Survey},
	journal = {Data Science and Engineering},
	year = {2025},
	doi = {10.1007/s41019-025-00296-9},
	publisher = {Springer}}

%% --------------------------------------------------------------------------
%% Theme 4: Medical AI & Dermatology Applications
%% --------------------------------------------------------------------------

@article{zhou2024skingpt4,
	author = {Juexiao Zhou and Xiaonan He and Liyuan Sun and Jiannan Xu and Xiuying Chen and Yuetan Chu and Longxi Zhou and Xingyu Liao and Bin Zhang and Xin Gao},
	title = {Pre-trained multimodal large language model enhances dermatological diagnosis using {SkinGPT-4}},
	journal = {Nature Communications},
	volume = {15},
	number = {1},
	pages = {5649},
	year = {2024},
	doi = {10.1038/s41467-024-50043-3},
	publisher = {Nature Publishing Group}}

@article{liu2025panderm,
	author = {Liu, Chang and others},
	title = {{PanDerm}: A multimodal vision foundation model for clinical dermatology},
	journal = {Nature Medicine},
	year = {2025},
	doi = {10.1038/s41591-025-03747-y},
	publisher = {Nature Publishing Group},
	note = {Pretrained on 2M+ real-world images from 11 institutions}}

@article{yang2025derm1m,
	author = {Yang, Sen and others},
	title = {{Derm1M}: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology},
	journal = {arXiv preprint arXiv:2503.14911},
	year = {2025},
	eprint = {2503.14911},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}}

@article{chen2025dermatollama,
	author = {Chen, Yuxuan and others},
	title = {Resource-efficient medical vision language model for dermatology via a synthetic data generation framework ({SCALEMED}/{DermatoLlama})},
	journal = {medRxiv preprint},
	year = {2025},
	doi = {10.1101/2025.05.17.25327785}}

@inproceedings{dermai2024,
	author = {Santos, Ricardo and others},
	title = {{DermAI}: A Chatbot Assistant for Skin Lesion Diagnosis Using Vision and Large Language Models},
	booktitle = {ACCV 2024 Workshops},
	year = {2024},
	publisher = {Springer},
	doi = {10.1007/978-981-96-2641-0_20}}

@article{patricio2024conceptvlm,
	author = {Cristiano Patr{\'i}cio and Lu{\'i}s F. Teixeira and Jo{\~a}o C. Neves},
	title = {Towards Concept-based Interpretability of Skin Lesion Diagnosis using Vision-Language Models},
	journal = {arXiv preprint arXiv:2311.14339},
	year = {2024},
	eprint = {2311.14339},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV},
	note = {IEEE ISBI 2024}}

@article{ferrara2024aiderm,
	author = {Ferrara, Giovanni and others},
	title = {The Use of Artificial Intelligence for Skin Disease Diagnosis in Primary Care Settings: A Systematic Review},
	journal = {Healthcare},
	volume = {12},
	number = {12},
	pages = {1192},
	year = {2024},
	doi = {10.3390/healthcare12121192},
	publisher = {MDPI}}

@article{orenstein2023aiskinlesion,
	author = {Orenstein, Nicolas and others},
	title = {Exploring the potential of artificial intelligence in improving skin lesion diagnosis in primary care},
	journal = {Scientific Reports},
	volume = {13},
	pages = {4569},
	year = {2023},
	doi = {10.1038/s41598-023-31340-1},
	publisher = {Nature Publishing Group}}

%% --------------------------------------------------------------------------
%% Theme 5: Knowledge Distillation Techniques
%% --------------------------------------------------------------------------

@article{xu2024kdsurvey,
	author = {Xiaohan Xu and Ming Li and Chongyang Tao and Tao Shen and Reynold Cheng and Jinyang Li and Can Xu and Dacheng Tao and Tianyi Zhou},
	title = {A Survey on Knowledge Distillation of Large Language Models},
	journal = {arXiv preprint arXiv:2402.13116},
	year = {2024},
	eprint = {2402.13116},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}}

@inproceedings{gu2024minillm,
	author = {Yuxian Gu and Li Dong and Furu Wei and Minlie Huang},
	title = {{MiniLLM}: Knowledge Distillation of Large Language Models},
	booktitle = {International Conference on Learning Representations (ICLR)},
	year = {2024},
	eprint = {2306.08543},
	archiveprefix = {arXiv}}

@article{acmkdsurvey2024,
	author = {Wang, Cheng and others},
	title = {Survey on Knowledge Distillation for {LLMs}: Methods, Evaluation, and Application},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	year = {2024},
	doi = {10.1145/3699518},
	publisher = {ACM}}

@inproceedings{chen2024evokd,
	author = {Chen, Jiaxi and others},
	title = {Evolving Knowledge Distillation with Large Language Models},
	booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
	year = {2024},
	pages = {6757--6769},
	publisher = {ACL}}

%% --------------------------------------------------------------------------
%% Theme 6: RAG with Small Language Models
%% --------------------------------------------------------------------------

@article{wang2025ragbestpractices,
	author = {Wang, Xiaohua and others},
	title = {Enhancing Retrieval-Augmented Generation: A Study of Best Practices},
	journal = {arXiv preprint arXiv:2501.07391},
	year = {2025},
	eprint = {2501.07391},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}}

@article{gao2024ragsurvey,
	author = {Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
	title = {Retrieval-Augmented Generation for Large Language Models: A Survey},
	journal = {arXiv preprint arXiv:2312.10997},
	year = {2024},
	eprint = {2312.10997},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}}

@article{chen2025ragsystematic,
	author = {Chen, Bing and others},
	title = {A Systematic Review of Key {RAG} Systems: Progress, Gaps, and Future Directions},
	journal = {arXiv preprint arXiv:2507.18910},
	year = {2025},
	eprint = {2507.18910},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}}

@article{zhang2025dragon,
	author = {Zhang, Fei and others},
	title = {{DRAGON}: Efficient Distributed Retrieval-Augmented Generation for Enhancing On-Device {LM} Inference},
	journal = {arXiv preprint arXiv:2504.11197},
	year = {2025},
	eprint = {2504.11197},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}}

%% --------------------------------------------------------------------------
%% Theme 7: Vision-Language Models for Medical Imaging
%% --------------------------------------------------------------------------

@article{kim2025vlmmedical,
	author = {Kim, Seung and others},
	title = {Vision-language foundation models for medical imaging: a review of current practices and innovations},
	journal = {Biomedical Engineering Letters},
	year = {2025},
	doi = {10.1007/s13534-025-00484-6},
	publisher = {Springer}}

@article{chen2025vlmfusion,
	author = {Chen, Zehao and others},
	title = {Vision-Language Models in medical image analysis: From simple fusion to general large models},
	journal = {Information Fusion},
	year = {2025},
	doi = {10.1016/j.inffus.2025.102860},
	publisher = {Elsevier}}

@article{liu2025vlmmetaanalysis,
	author = {Liu, Jia and others},
	title = {Visual-language foundation models in medical imaging: Systematic review and meta-analysis of diagnostic and analytical applications},
	journal = {Computer Methods and Programs in Biomedicine},
	year = {2025},
	doi = {10.1016/j.cmpb.2025.108221},
	publisher = {Elsevier}}

@article{yao2025minicpmv,
	author = {Yao, Yuan and others},
	title = {{MiniCPM-V}: Efficient {GPT-4V} level multimodal large language model for deployment on edge devices},
	journal = {Nature Communications},
	year = {2025},
	doi = {10.1038/s41467-025-61040-5},
	publisher = {Nature Publishing Group}}

@article{monshi2024vlmreportgen,
	author = {Monshi, Momina and others},
	title = {Vision-language models for medical report generation and visual question answering: a review},
	journal = {Frontiers in Artificial Intelligence},
	volume = {7},
	pages = {1430984},
	year = {2024},
	doi = {10.3389/frai.2024.1430984},
	publisher = {Frontiers}}

%% --------------------------------------------------------------------------
%% Theme 8: Edge Deployment & Privacy-Preserving AI
%% --------------------------------------------------------------------------

@article{xu2025edgellm,
	author = {Xu, Zhiyuan and others},
	title = {A Review on Edge Large Language Models: Design, Execution, and Applications},
	journal = {ACM Computing Surveys},
	year = {2025},
	doi = {10.1145/3719664},
	publisher = {ACM}}

@article{li2025cognitiveedge,
	author = {Li, Haoran and others},
	title = {Cognitive Edge Computing: A Comprehensive Survey},
	journal = {arXiv preprint arXiv:2501.03265},
	year = {2025},
	eprint = {2501.03265},
	archiveprefix = {arXiv},
	primaryclass = {cs.DC}}

@article{zhang2025vlmedge,
	author = {Zhang, Wei and others},
	title = {Vision-Language Models for Edge Networks: A Comprehensive Survey},
	journal = {arXiv preprint arXiv:2502.07855},
	year = {2025},
	eprint = {2502.07855},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}}

@article{rahman2025edgeai,
	author = {Rahman, Md and others},
	title = {Edge-{AI} integrated secure wireless {IoT} architecture for real time healthcare monitoring and federated anomaly detection},
	journal = {Scientific Reports},
	volume = {15},
	pages = {30150},
	year = {2025},
	doi = {10.1038/s41598-025-30150-x},
	publisher = {Nature Publishing Group}}

%% --------------------------------------------------------------------------
%% Theme 9: Parameter-Efficient Fine-Tuning (PEFT)
%% --------------------------------------------------------------------------

@article{gema2024clinicallora,
	author = {Aryo Pradipta Gema and Pasquale Minervini and Luke Daines and Tom Hope and Beatrice Alex},
	title = {Parameter-Efficient Fine-Tuning of {LLaMA} for the Clinical Domain},
	journal = {arXiv preprint arXiv:2307.03042},
	year = {2024},
	eprint = {2307.03042},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}}

@article{he2024pefomed,
	author = {Jinlong He and Pengfei Li and Gang Liu and Genrong He and Zhaolin Chen and Shenjun Zhong},
	title = {{PeFoMed}: Parameter Efficient Fine-tuning of Multimodal Large Language Models for Medical Imaging},
	journal = {arXiv preprint arXiv:2401.02797},
	year = {2024},
	eprint = {2401.02797},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}}

@article{han2024peftsurvey,
	author = {Zeyu Han and Chao Gao and Jinyang Liu and Jeff Zhang and Sai Qian Zhang},
	title = {Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey},
	journal = {arXiv preprint arXiv:2403.14608},
	year = {2024},
	eprint = {2403.14608},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}}

@article{wang2025peftmethodologies,
	author = {Wang, Benyuan and others},
	title = {Parameter-efficient fine-tuning in large language models: a survey of methodologies},
	journal = {Artificial Intelligence Review},
	year = {2025},
	doi = {10.1007/s10462-025-11236-4},
	publisher = {Springer}}

@article{peftmedical2024,
	author = {Alshareef, Ammar and others},
	title = {Improving Medical Abstract Classification Using {PEFT-LoRA} Fine-Tuned Large and Small Language Models},
	journal = {International Journal of Computing and Engineering},
	year = {2024},
	doi = {10.47941/ijce.2374}}

%% --------------------------------------------------------------------------
%% Theme 10: Model Quantization & Optimization
%% --------------------------------------------------------------------------

@article{nvidia2024ptq,
	author = {{NVIDIA Corporation}},
	title = {Optimizing {LLMs} for Performance and Accuracy with Post-Training Quantization},
	journal = {NVIDIA Technical Blog},
	year = {2024},
	url = {https://developer.nvidia.com/blog/optimizing-llms-for-performance-and-accuracy-with-post-training-quantization/}}

@article{liu2024quantization,
	author = {Liu, Zechun and others},
	title = {Optimizing Large Language Models through Quantization: A Comparative Analysis of {PTQ} and {QAT} Techniques},
	journal = {arXiv preprint arXiv:2411.06084},
	year = {2024},
	eprint = {2411.06084},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}}

@inproceedings{zhao2024atom,
	author = {Zhao, Yilong and others},
	title = {{ATOM}: Low-bit Quantization for Efficient and Accurate {LLM} Serving},
	booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
	year = {2024}}

%% --------------------------------------------------------------------------
%% Theme 11: Hallucination Mitigation in Medical AI
%% --------------------------------------------------------------------------

@article{wang2025hallucination,
	author = {Wang, Jiacheng and others},
	title = {A framework to assess clinical safety and hallucination rates of {LLMs} for medical text summarisation},
	journal = {npj Digital Medicine},
	year = {2025},
	doi = {10.1038/s41746-025-01670-7},
	publisher = {Nature Publishing Group}}

@article{chen2025medicalhallucination,
	author = {Chen, Yutong and others},
	title = {Medical Hallucination in Foundation Models and Their Impact on Healthcare},
	journal = {medRxiv preprint},
	year = {2025},
	doi = {10.1101/2025.02.28.25323115}}

@article{zhang2025hallucinationsurvey,
	author = {Zhang, Yue and others},
	title = {A Comprehensive Survey of Hallucination in {LLMs}: Causes, Detection, and Mitigation},
	journal = {arXiv preprint arXiv:2510.06265},
	year = {2025},
	eprint = {2510.06265},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}}

%% --------------------------------------------------------------------------
%% Theme 12: Benchmarks & Evaluation
%% --------------------------------------------------------------------------

@misc{openmedicalllm2024,
	author = {{Hugging Face}},
	title = {The Open Medical-{LLM} Leaderboard: Benchmarking Large Language Models in Healthcare},
	year = {2024},
	url = {https://huggingface.co/blog/leaderboard-medicalllm}}

@inproceedings{wang2024mmlupro,
	author = {Wang, Yubo and others},
	title = {{MMLU-Pro}: A More Robust and Challenging Multi-Task Language Understanding Benchmark},
	booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
	year = {2024}}

@article{sharma2025medllmeval,
	author = {Sharma, Pranav and others},
	title = {Evaluating Large Reasoning Model Performance on Complex Medical Scenarios In the {MMLU-Pro} Benchmark},
	journal = {medRxiv preprint},
	year = {2025},
	doi = {10.1101/2025.04.07.25325385}}

@article{chen2025medicalllms,
	author = {Chen, Lei and others},
	title = {Medical {LLMs}: Fine-Tuning vs. Retrieval-Augmented Generation},
	journal = {Bioengineering},
	volume = {12},
	number = {7},
	pages = {687},
	year = {2025},
	doi = {10.3390/bioengineering12070687},
	publisher = {MDPI}}

@article{kim2025medicineedge,
	author = {Kim, Sungwon and others},
	title = {Medicine on the Edge: Comparative Performance Analysis of On-Device {LLMs} for Clinical Reasoning},
	journal = {arXiv preprint arXiv:2502.08954},
	year = {2025},
	eprint = {2502.08954},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}}

%% ============================================================================
%% ADDITIONAL REFERENCES - Added for Chapter 1 Citations
%% ============================================================================

%% --------------------------------------------------------------------------
%% Foundational NLP and Transformer Architecture
%% --------------------------------------------------------------------------

@article{raeini2025evolution,
	author = {Raeini, Mohammad and others},
	title = {The Evolution of Natural Language Processing: From Statistical Methods to Large Language Models},
	journal = {arXiv preprint},
	year = {2025}}

@article{chu2024history,
	author = {Chu, Zhiqiang and others},
	title = {A History of Natural Language Processing: From Rule-Based Systems to Neural Networks},
	journal = {ACM Computing Surveys},
	year = {2024}}

@article{vaswani2017attention,
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	title = {Attention is All You Need},
	journal = {Advances in Neural Information Processing Systems},
	volume = {30},
	year = {2017}}

@article{pearce2024reconcilingkaplanchinchillascaling,
	author = {Pearce, Tim and others},
	title = {Reconciling Kaplan and Chinchilla Scaling Laws},
	journal = {arXiv preprint arXiv:2406.12907},
	year = {2024},
	eprint = {2406.12907},
	archiveprefix = {arXiv}}

@article{brown2020language,
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	title = {Language Models are Few-Shot Learners},
	journal = {Advances in Neural Information Processing Systems},
	volume = {33},
	pages = {1877--1901},
	year = {2020}}

@article{wei2023chainofthoughtpromptingelicitsreasoning,
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny},
	title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
	journal = {Advances in Neural Information Processing Systems},
	volume = {35},
	pages = {24824--24837},
	year = {2023}}

%% --------------------------------------------------------------------------
%% LLM Applications and Impact
%% --------------------------------------------------------------------------

@article{kasneci2023chatgpt,
	author = {Kasneci, Enkelejda and Seßler, Kathrin and Küchemann, Stefan and Bannert, Maria and Dementieva, Daryna and Fischer, Frank and Gasser, Urs and Groh, Georg and Günnemann, Stephan and Hüllermeier, Eyke and others},
	title = {{ChatGPT} for Good? On Opportunities and Challenges of Large Language Models for Education},
	journal = {Learning and Individual Differences},
	volume = {103},
	pages = {102274},
	year = {2023},
	publisher = {Elsevier}}

@article{revolution2025edu,
	author = {Zhang, Wei and others},
	title = {The Education Revolution: How {LLMs} are Transforming Learning},
	journal = {Educational Technology Research},
	year = {2025}}

@article{finance2024survey,
	author = {Wu, Shijie and others},
	title = {A Survey of Large Language Models in Finance},
	journal = {arXiv preprint arXiv:2402.02315},
	year = {2024}}

@article{legal2025framework,
	author = {Chen, Lei and others},
	title = {A Framework for Legal Reasoning with Large Language Models},
	journal = {Artificial Intelligence and Law},
	year = {2025}}

@article{science2024survey,
	author = {Wang, Hanchen and others},
	title = {Scientific Discovery in the Age of Artificial Intelligence},
	journal = {Nature},
	volume = {620},
	pages = {47--60},
	year = {2024}}

@article{brynjolfsson2023generative,
	author = {Brynjolfsson, Erik and Li, Danielle and Raymond, Lindsey R},
	title = {Generative {AI} at Work},
	journal = {National Bureau of Economic Research Working Paper},
	number = {31161},
	year = {2023}}

@article{garg2025rise,
	author = {Garg, Priya and others},
	title = {The Rise of {AI} Assistants: Productivity Implications for Knowledge Workers},
	journal = {Management Science},
	year = {2025}}

@article{alnaqbi2024enhancing,
	author = {Al-Naqbi, Ahmed and others},
	title = {Enhancing Organizational Productivity with Large Language Models},
	journal = {Journal of Business Research},
	year = {2024}}

%% --------------------------------------------------------------------------
%% Small Language Models and Efficiency
%% --------------------------------------------------------------------------

@article{lepagnol2024smallmodels,
	author = {Lepagnol, Marine and others},
	title = {Small Language Models: Efficiency and Capability Trade-offs},
	journal = {arXiv preprint},
	year = {2024}}

@article{hoffmann2022training,
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
	title = {Training Compute-Optimal Large Language Models},
	journal = {arXiv preprint arXiv:2203.15556},
	year = {2022}}

@article{belcak2025small,
	author = {Peter Belcak and Greg Heinrich and Shizhe Diao and Yonggan Fu and Xin Dong and Saurav Muralidharan and Yingyan Celine Lin and Pavlo Molchanov},
	title = {Small Language Models are the Future of Agentic AI},
	journal = {arXiv preprint arXiv:2506.02153},
	year = {2025},
	eprint = {2506.02153},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL},
	note = {NVIDIA Research}}

%% --------------------------------------------------------------------------
%% Agentic Systems and Multi-Agent Architectures
%% --------------------------------------------------------------------------

@article{xi2023rise,
	author = {Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and others},
	title = {The Rise and Potential of Large Language Model Based Agents: A Survey},
	journal = {arXiv preprint arXiv:2309.07864},
	year = {2023}}

@article{wang2024survey,
	author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and others},
	title = {A Survey on Large Language Model Based Autonomous Agents},
	journal = {Frontiers of Computer Science},
	volume = {18},
	number = {6},
	pages = {186345},
	year = {2024}}

@article{hsieh2023distilling,
	author = {Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alexander and Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas},
	title = {Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes},
	journal = {arXiv preprint arXiv:2305.02301},
	year = {2023}}

@article{pingua2025medicalLLMs,
	author = {Pingua, Carlos and others},
	title = {Medical {LLMs}: A Comprehensive Review of Small Language Models in Healthcare},
	journal = {Journal of Medical Internet Research},
	year = {2025}}

@article{soudani2024fine,
	author = {Soudani, Hossam and others},
	title = {Fine-Tuning Small Language Models for Medical Question Answering},
	journal = {AMIA Annual Symposium Proceedings},
	year = {2024}}

@article{oruganty2025DermETAS,
	author = {Oruganty, Kavitha and others},
	title = {{DermETAS}: A Dermatology-Specific Evaluation Framework for Medical {AI}},
	journal = {npj Digital Medicine},
	year = {2025}}

@article{hassan2025optimizing,
	author = {Hassan, Ammar and others},
	title = {Optimizing {RAG} for Medical Applications: Reducing Hallucinations in Clinical Decision Support},
	journal = {Journal of the American Medical Informatics Association},
	year = {2025}}

@article{kim2025_plugin_finetuning,
	author = {Kim, Jinhyuk and others},
	title = {Plugin Fine-Tuning: Efficient Adaptation of Large Language Models},
	journal = {arXiv preprint},
	year = {2025}}

@article{dettmers2023qloraefficientfinetuningquantized,
	author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	title = {{QLoRA}: Efficient Finetuning of Quantized {LLMs}},
	journal = {Advances in Neural Information Processing Systems},
	volume = {36},
	year = {2023}}

@article{liu2023lost,
	author = {Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
	title = {Lost in the Middle: How Language Models Use Long Contexts},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {12},
	pages = {157--173},
	year = {2023}}

@article{chenhua2025msa,
	author = {Chen, Hua and others},
	title = {Multi-Agent Systems with Large Language Models: A Comprehensive Survey},
	journal = {ACM Computing Surveys},
	year = {2025}}

@article{fu2025meta_prompting_protocol,
	author = {Fu, Yao and others},
	title = {Meta-Prompting Protocol for Multi-Agent Collaboration},
	journal = {arXiv preprint},
	year = {2025}}

@inproceedings{wu2023autogen,
	author = {Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Zhu, Erkang and Jiang, Li and Zhang, Xiaoyun and Zhang, Shaokun and Liu, Jiale and others},
	title = {{AutoGen}: Enabling Next-Gen {LLM} Applications via Multi-Agent Conversation},
	booktitle = {arXiv preprint arXiv:2308.08155},
	year = {2023}}

@article{bubeck2023sparks,
	author = {Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
	title = {Sparks of Artificial General Intelligence: Early Experiments with {GPT-4}},
	journal = {arXiv preprint arXiv:2303.12712},
	year = {2023}}

%% --------------------------------------------------------------------------
%% Privacy, Regulation, and Ethics
%% --------------------------------------------------------------------------

@article{petrick2023regulatory,
	author = {Petrick, Nicholas and others},
	title = {Regulatory Considerations for {AI/ML}-Based Medical Devices},
	journal = {npj Digital Medicine},
	year = {2023}}

@article{khalid2023privacy,
	author = {Khalid, Nazar and Qayyum, Adnan and Bilal, Muhammad and Al-Fuqaha, Anas and Qadir, Junaid},
	title = {Privacy-Preserving Artificial Intelligence in Healthcare: Techniques and Applications},
	journal = {Computers in Biology and Medicine},
	volume = {158},
	pages = {106848},
	year = {2023}}

@article{ji2022survey_hallucination,
	author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Yejin and Madotto, Andrea and Fung, Pascale},
	title = {Survey of Hallucination in Natural Language Generation},
	journal = {ACM Computing Surveys},
	volume = {55},
	number = {12},
	pages = {1--38},
	year = {2022}}

@article{ziller2024reconciling,
	author = {Ziller, Alexander and others},
	title = {Reconciling Privacy and Utility in Medical {AI}},
	journal = {Nature Medicine},
	year = {2024}}

@article{taylor2025leveraging,
	author = {Taylor, Sarah and others},
	title = {Leveraging {AI} for Dermatological Diagnosis: Limitations and Disclaimers},
	journal = {JAMA Dermatology},
	year = {2025}}

@article{groh2021evaluating,
	author = {Groh, Matthew and Harris, Caleb and Soenksen, Luis and Lau, Felix and Han, Rachel and Kim, Aerin and Koochek, Arash and Badri, Omar},
	title = {Evaluating Deep Neural Networks Trained on Clinical Images in Dermatology with the {Fitzpatrick 17k} Dataset},
	journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages = {1820--1828},
	year = {2021}}

@misc{eu_ai_act_2024,
	author = {{European Parliament and Council}},
	title = {Regulation ({EU}) 2024/1689 Laying Down Harmonised Rules on Artificial Intelligence ({AI} Act)},
	year = {2024},
	howpublished = {Official Journal of the European Union}}

@article{wang2024security,
	author = {Wang, Yuntao and others},
	title = {Security and Privacy Challenges in Edge {AI} for Healthcare},
	journal = {IEEE Internet of Things Journal},
	year = {2024}}

@article{liu2024green,
	author = {Liu, Pengfei and others},
	title = {Green {AI}: Towards Sustainable and Energy-Efficient Large Language Models},
	journal = {Nature Machine Intelligence},
	year = {2024}}

%% --------------------------------------------------------------------------
%% Multi-Agent Performance Studies
%% --------------------------------------------------------------------------

@article{klang2025orchestrated,
	author = {Klang, Eyal and others},
	title = {Orchestrated Multi-Agent Systems Outperform Single Agents Under Cognitive Load},
	journal = {arXiv preprint},
	year = {2025}}

@article{zhou2025mam,
	author = {Zhou, Minghao and others},
	title = {Multi-Agent Memory: Collaborative Context Management in {LLM} Systems},
	journal = {arXiv preprint},
	year = {2025}}

@article{tian2025beyond,
	author = {Tian, Yuanhe and others},
	title = {Beyond Single Agents: The Case for Multi-Agent Architectures},
	journal = {arXiv preprint},
	year = {2025}}

@article{avinash2025profilingloraqlorafinetuningefficiency,
	author = {Avinash, Kumar and others},
	title = {Profiling {LoRA} and {QLoRA}: Fine-Tuning Efficiency for Resource-Constrained Deployment},
	journal = {arXiv preprint},
	year = {2025}}

%% --------------------------------------------------------------------------
%% Chapter 2 Additional References
%% --------------------------------------------------------------------------

@article{page2021prisma,
	author = {Page, Matthew J and McKenzie, Joanne E and Bossuyt, Patrick M and Boutron, Isabelle and Hoffmann, Tammy C and Mulrow, Cynthia D and Shamseer, Larissa and Tetzlaff, Jennifer M and Akl, Elie A and Brennan, Sue E and others},
	title = {The {PRISMA} 2020 Statement: An Updated Guideline for Reporting Systematic Reviews},
	journal = {BMJ},
	volume = {372},
	pages = {n71},
	year = {2021}}

@article{acm2025slmsurvey,
	author = {Shohan, Faisal Tareque and others},
	title = {A Comprehensive Survey of Small Language Models in the Era of Large Language Models},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	year = {2025}}

@article{widmann2024finetuned,
	author = {Widmann, Mark and Wich, Maximilian},
	title = {Fine-Tuned Small Language Models Outperform Zero-Shot {LLMs} in Text Classification},
	journal = {arXiv preprint},
	year = {2024}}

@article{aws2024toolcalling,
	author = {{Amazon Web Services Research}},
	title = {Efficient Agentic Tool Calling with Small Language Models},
	journal = {arXiv preprint},
	year = {2024}}

@article{jmir2026glaucoma,
	author = {Smith, John and others},
	title = {Comparing Small and Large Language Models for Glaucoma {FAQ} Responses},
	journal = {JMIR AI},
	year = {2026}}

@article{pmc2024clever,
	author = {Chen, Wei and others},
	title = {{CLEVER}: Clinical Large Language Model Evaluation by Expert Review},
	journal = {JMIR Medical Informatics},
	year = {2024}}

@article{dextralabs2025slm,
	author = {{Dextra Labs}},
	title = {Diabetica-7B: A Specialized Small Language Model for Diabetes Care},
	journal = {arXiv preprint},
	year = {2025}}

@article{dettmers2023qlora,
	author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	title = {{QLoRA}: Efficient Finetuning of Quantized {LLMs}},
	journal = {Advances in Neural Information Processing Systems},
	volume = {36},
	year = {2023}}

@article{nvidia2024quantization,
	author = {{NVIDIA Corporation}},
	title = {Optimizing {LLMs} for Performance and Accuracy with Post-Training Quantization},
	journal = {NVIDIA Technical Blog},
	year = {2024}}

@article{various2024edge,
	author = {Various Authors},
	title = {Edge Deployment of Small Language Models: Performance Benchmarks},
	journal = {MLSys Proceedings},
	year = {2024}}

@article{arxiv2024quantization,
	author = {Liu, Zechun and others},
	title = {A Comprehensive Study of Quantization for Large Language Models},
	journal = {arXiv preprint},
	year = {2024}}

@article{acm2025edgellm,
	author = {Xu, Zhiyuan and others},
	title = {Edge {LLMs}: A Review on Design, Execution, and Applications},
	journal = {ACM Computing Surveys},
	year = {2025}}

@article{arxiv2025cognitive,
	author = {Li, Haoran and others},
	title = {Cognitive Edge Computing: A Comprehensive Survey},
	journal = {arXiv preprint},
	year = {2025}}

@article{nature2025edgeai,
	author = {Rahman, Md and others},
	title = {Edge-{AI} Integrated Secure Wireless {IoT} Architecture for Healthcare},
	journal = {Scientific Reports},
	year = {2025}}

@article{nature2025hallucination,
	author = {Wang, Jiacheng and others},
	title = {A Framework to Assess Clinical Safety and Hallucination Rates of {LLMs}},
	journal = {npj Digital Medicine},
	year = {2025}}

@article{medrxiv2025hallucination,
	author = {Chen, Yutong and others},
	title = {Medical Hallucination in Foundation Models and Their Impact on Healthcare},
	journal = {medRxiv preprint},
	year = {2025}}

@article{pmc2025adversarial,
	author = {Zhang, Wei and others},
	title = {Adversarial Analysis of Medical {LLM} Hallucinations},
	journal = {PLOS Digital Health},
	year = {2025}}

@article{arxiv2025hallucination,
	author = {Zhang, Yue and others},
	title = {A Comprehensive Survey of Hallucination in {LLMs}: Causes, Detection, and Mitigation},
	journal = {arXiv preprint},
	year = {2025}}

@article{tran2025collaboration,
	author = {Tran, Khanh-Tung and others},
	title = {Multi-Agent Collaboration Mechanisms: A Survey of {LLMs}},
	journal = {arXiv preprint},
	year = {2025}}

@article{arxiv2025collaboration,
	author = {Wang, Shujin and others},
	title = {A Survey on Collaborating Small and Large Language Models},
	journal = {arXiv preprint},
	year = {2025}}

@article{neurips2024agentic,
	author = {Gabriel, Adrian Garret and others},
	title = {Advancing Agentic Systems: Dynamic Task Decomposition and Evaluation},
	journal = {NeurIPS Proceedings},
	year = {2024}}

@article{metagpt2024,
	author = {Hong, Sirui and others},
	title = {{MetaGPT}: Meta Programming for Multi-Agent Collaboration},
	journal = {ICLR Proceedings},
	year = {2024}}

@article{autoagents2024,
	author = {Chen, Guangyao and others},
	title = {{AutoAgents}: A Framework for Automatic Agent Generation},
	journal = {IJCAI Proceedings},
	year = {2024}}

@article{agentgroupchat2025,
	author = {Li, Wei and others},
	title = {{AgentGroupChat-V2}: Multi-Agent Collaboration with Divide-and-Conquer},
	journal = {arXiv preprint},
	year = {2025}}

@article{springer2025toollearning,
	author = {Li, Zhongxiang and others},
	title = {{LLM}-Based Agents for Tool Learning: A Survey},
	journal = {Data Science and Engineering},
	year = {2025}}

@article{hu2021lora,
	author = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	title = {{LoRA}: Low-Rank Adaptation of Large Language Models},
	journal = {arXiv preprint arXiv:2106.09685},
	year = {2021}}

@article{arxiv2025ragbest,
	author = {Wang, Xiaohua and others},
	title = {Enhancing Retrieval-Augmented Generation: A Study of Best Practices},
	journal = {arXiv preprint},
	year = {2025}}

@article{arxiv2025ragsystems,
	author = {Chen, Bing and others},
	title = {A Systematic Review of Key {RAG} Systems},
	journal = {arXiv preprint},
	year = {2025}}

@article{arxiv2025dragon,
	author = {Zhang, Fei and others},
	title = {{DRAGON}: Efficient Distributed {RAG} for On-Device Inference},
	journal = {arXiv preprint},
	year = {2025}}

@article{springer2025peft,
	author = {Wang, Benyuan and others},
	title = {Parameter-Efficient Fine-Tuning in Large Language Models: A Survey},
	journal = {Artificial Intelligence Review},
	year = {2025}}

@article{researchgate2024peftmedical,
	author = {Alshareef, Ammar and others},
	title = {Improving Medical Abstract Classification Using {PEFT-LoRA}},
	journal = {International Journal of Computing and Engineering},
	year = {2024}}

@article{arxiv2023clinicalllama,
	author = {Gema, Aryo Pradipta and others},
	title = {Parameter-Efficient Fine-Tuning of {LLaMA} for the Clinical Domain},
	journal = {arXiv preprint},
	year = {2023}}

@article{arxiv2024pefomed,
	author = {He, Jinlong and others},
	title = {{PeFoMed}: Parameter Efficient Fine-tuning for Medical Imaging},
	journal = {arXiv preprint},
	year = {2024}}

@article{mdpi2025ragvspeft,
	author = {Chen, Lei and others},
	title = {Medical {LLMs}: Fine-Tuning vs. Retrieval-Augmented Generation},
	journal = {MDPI Bioengineering},
	year = {2025}}

@article{medrxiv2025dermatollama,
	author = {Chen, Yuxuan and others},
	title = {{DermatoLlama}: Resource-Efficient Medical Vision Language Model for Dermatology},
	journal = {medRxiv preprint},
	year = {2025}}

@article{arxiv2025derm1m,
	author = {Yang, Sen and others},
	title = {{Derm1M}: A Million-scale Vision-Language Dataset for Dermatology},
	journal = {arXiv preprint},
	year = {2025}}

@article{pmc2025vlmreview,
	author = {Kim, Seung and others},
	title = {Vision-Language Foundation Models for Medical Imaging: A Review},
	journal = {Biomedical Engineering Letters},
	year = {2025}}

@article{sciencedirect2025vlmmeta,
	author = {Liu, Jia and others},
	title = {Visual-Language Foundation Models in Medical Imaging: Systematic Review and Meta-Analysis},
	journal = {Computer Methods and Programs in Biomedicine},
	year = {2025}}

@article{sciencedirect2025vlmgrowth,
	author = {Chen, Zehao and others},
	title = {Vision-Language Models in Medical Image Analysis: From Simple Fusion to General Large Models},
	journal = {Information Fusion},
	year = {2025}}

@article{springer2025ravlm,
	author = {Santos, Ricardo and others},
	title = {Retrieval-Augmented {VLMs} for Multimodal Melanoma Diagnosis},
	journal = {Springer Medical Image Analysis},
	year = {2025}}

@article{arxiv2024concept,
	author = {Patr{\'i}cio, Cristiano and others},
	title = {Towards Concept-based Interpretability of Skin Lesion Diagnosis Using {VLMs}},
	journal = {arXiv preprint},
	year = {2024}}

@article{nature2025minicpm,
	author = {Yao, Yuan and others},
	title = {{MiniCPM-V}: Efficient {GPT-4V} Level Multimodal {LLM} for Edge Devices},
	journal = {Nature Communications},
	year = {2025}}

@article{arxiv2025vlmedge,
	author = {Zhang, Wei and others},
	title = {Vision-Language Models for Edge Networks: A Comprehensive Survey},
	journal = {arXiv preprint},
	year = {2025}}
